{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8644700a",
   "metadata": {},
   "source": [
    "## A customized DQN implementation\n",
    "\n",
    "A small-scaled DQN implementation for fast prototyping and playing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bea440d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.7/Project.toml`\n",
      " \u001b[90m [b8865327] \u001b[39m\u001b[92m+ UnicodePlots v2.10.3\u001b[39m\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.7/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg;\n",
    "# uncomment the following if you have not installed them\n",
    "# Pkg.add(\"ReinforcementLearning\");\n",
    "# Pkg.add(\"Flux\");\n",
    "# Pkg.add(\"StableRNGs\");\n",
    "# Pkg.add(\"Distributions\");]\n",
    "# Pkg.add(\"UnicodePlots\")\n",
    "using Flux: InvDecay;\n",
    "using ReinforcementLearning;\n",
    "using StableRNGs;\n",
    "using Flux;\n",
    "using Flux.Losses;\n",
    "using Distributions;\n",
    "using UnicodePlots:lineplot, lineplot!\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e399c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed and env\n",
    "seed = 245;\n",
    "rng = StableRNG(seed);\n",
    "env = CartPoleEnv(; T = Float32);\n",
    "ns, na = length(state_space(env)), length(action_space(env));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4125c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_learner (generic function with 1 method)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model, policy, loss, update step\n",
    "learner = Chain(\n",
    "    Dense(ns, 128, relu; init = glorot_uniform(rng)),\n",
    "    Dense(128, 128, relu; init = glorot_uniform(rng)),\n",
    "    Dense(128, na; init = glorot_uniform(rng)),\n",
    ") |> gpu;\n",
    "optimizer = ADAM();\n",
    "\n",
    "function GreedyPolicy(states, learner)\n",
    "    logits = learner(states)\n",
    "    actions = mapslices(argmax, logits, dims=1)\n",
    "    return actions\n",
    "end\n",
    "\n",
    "function EpsilonGreedyPolicy(states, learner, t_current, t_max)\n",
    "    ϵ_min = 0.005\n",
    "    ϵ = max(1-t_current/t_max, ϵ_min)\n",
    "    random_number = rand(Uniform(0,1))\n",
    "    if random_number > ϵ\n",
    "        action = GreedyPolicy(states, learner)\n",
    "    else\n",
    "        action = rand(rng, 1:2)\n",
    "    end\n",
    "    return action\n",
    "end\n",
    "\n",
    "function value_loss(batch)\n",
    "    # TODO: improve the inefficient loss calculation\n",
    "    num_sample = length(batch[\"actions\"])\n",
    "    loss = 0\n",
    "    γ = 0.96\n",
    "    q_values = learner(batch[\"states\"])\n",
    "    next_values = findmax(learner(batch[\"next_states\"]); dims=1)[1]\n",
    "    for i in 1:length(batch[\"actions\"])\n",
    "        target = Flux.Zygote.ignore() do\n",
    "            batch[\"rewards\"] + γ*next_values\n",
    "        end\n",
    "        loss = loss + mse(q_values[batch[\"actions\"][i],i], target)\n",
    "    end\n",
    "    return loss/num_sample\n",
    "end\n",
    "\n",
    "function update_learner(learner, batch)\n",
    "    grad = Flux.gradient(Flux.params(learner)) do\n",
    "        value_loss(batch)\n",
    "    end\n",
    "    Flux.update!(optimizer, Flux.params(learner), grad)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556884b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interact with env to collect data and do the update steps\n",
    "policy = EpsilonGreedyPolicy\n",
    "stop_criterion = StopAfterEpisode(15000)\n",
    "\n",
    "# ss = nothing\n",
    "# aa = nothing\n",
    "# rr = nothing\n",
    "# nst = nothing\n",
    "# bb = nothing\n",
    "# aa_idx = nothing\n",
    "total_rewards = Array{Float64}(undef, 1, 1)\n",
    "step_counter = 0\n",
    "max_step = 5e5\n",
    "\n",
    "while true\n",
    "    reset!(env)\n",
    "    episode_reward = 0\n",
    "    states = Array(state(env))\n",
    "    actions = Array{Int32}(undef, 1, 1)\n",
    "    rewards = Array{Float64}(undef, 1, 1)\n",
    "    \n",
    "    while !is_terminated(env)\n",
    "        #env |> policy |> env\n",
    "        action = policy(state(env), learner, step_counter, max_step)[1]\n",
    "        step_counter = step_counter +1\n",
    "        env(action)\n",
    "        \n",
    "        states = [states state(env)]\n",
    "        actions = [actions action]\n",
    "        rewards = [rewards reward(env)]\n",
    "        episode_reward += reward(env)\n",
    "        #stop_criterion(policy, env) && return # stop criterion: max episodes\n",
    "    end\n",
    "    # end of an episode\n",
    "    # processing the data\n",
    "    next_states = states[:,2:end]\n",
    "    states = states[:,1:end-1]\n",
    "    rewards = rewards[:,2:end]\n",
    "    actions = actions[:,2:end]\n",
    "    action_index = [(0,0)]\n",
    "    for i = 1:length(actions)\n",
    "        action_index = [action_index (actions[i],i)]\n",
    "    end\n",
    "    action_index = action_index[:,2:end]\n",
    "    batch = Dict(\"states\"=>states, \"actions\"=>actions, \"rewards\"=>rewards,\n",
    "                 \"next_states\"=>next_states, \"action_mask\"=>action_index)\n",
    "    \n",
    "    total_rewards = [total_rewards episode_reward]\n",
    "    step_counter >= max_step && break # stop criterion: max steps\n",
    "\n",
    "#     ss = states\n",
    "#     aa = actions\n",
    "#     rr = rewards\n",
    "#     nst = next_states\n",
    "#     aa_idx = action_index\n",
    "#     bb = batch\n",
    "    \n",
    "    # update steps\n",
    "    for i = 1:3\n",
    "        update_learner(learner, batch)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the episodes\n",
    "p = lineplot(total_rewards[2:end], title=\"Total reward per episode\", xlabel=\"Episode\", ylabel=\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7789a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15609b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e6892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98cd62d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
