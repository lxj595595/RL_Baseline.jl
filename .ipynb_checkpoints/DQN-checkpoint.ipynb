{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8644700a",
   "metadata": {},
   "source": [
    "## A customized DQN implementation\n",
    "\n",
    "A small-scaled DQN implementation for fast prototyping and playing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bea440d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg;\n",
    "#uncomment the following if you have not installed them\n",
    "# Pkg.add(\"ReinforcementLearning\");\n",
    "# Pkg.add(\"Flux\");\n",
    "# Pkg.add(\"StableRNGs\");\n",
    "# Pkg.add(\"Distributions\");\n",
    "# Pkg.add(\"UnicodePlots\");\n",
    "# Pkg.add(\"Zygote\")\n",
    "using Flux: InvDecay;\n",
    "using ReinforcementLearning;\n",
    "using StableRNGs;\n",
    "using Flux;\n",
    "using Flux.Losses;\n",
    "using Distributions;\n",
    "using UnicodePlots:lineplot, lineplot!\n",
    "using Statistics\n",
    "using Zygote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e399c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed and env\n",
    "seed = 245;\n",
    "rng = StableRNG(seed);\n",
    "env = CartPoleEnv(; T = Float32);\n",
    "ns, na = length(state_space(env)), length(action_space(env));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4125c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_learner (generic function with 1 method)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function GreedyPolicy(states, learner)\n",
    "    logits = learner(states)\n",
    "    actions = mapslices(argmax, logits, dims=1)\n",
    "    return actions\n",
    "end\n",
    "\n",
    "function EpsilonGreedyPolicy(states, learner, t_current, t_max)\n",
    "    ϵ_min = 0.005\n",
    "    ϵ = max(1-t_current/t_max, ϵ_min)\n",
    "    random_number = rand(Uniform(0,1))\n",
    "    if random_number > ϵ\n",
    "        action = GreedyPolicy(states, learner)\n",
    "    else\n",
    "        action = rand(rng, 1:2)\n",
    "    end\n",
    "    return action\n",
    "end\n",
    "\n",
    "function gather(q_values,actions)\n",
    "    num_samples = size(actions,2)\n",
    "    q_filtered = Array{Float64}(undef,1,1)\n",
    "    for i = 1:num_samples\n",
    "        q_filtered = [q_filtered q_values[actions[:,i],i]]\n",
    "    end\n",
    "    q_filtered = q_filtered[:,2:end]\n",
    "    return q_filtered\n",
    "end\n",
    "\n",
    "function value_loss(learner, batch)\n",
    "    # TODO: improve the inefficient loss calculation\n",
    "    num_sample = size(batch[\"actions\"],2)\n",
    "    loss = 0\n",
    "    γ = 0.96\n",
    "    q_values = learner(batch[\"states\"])\n",
    "    next_values = findmax(learner(batch[\"next_states\"]); dims=1)[1]\n",
    "################################################\n",
    "# implementation 1 (work)\n",
    "#     target = Flux.Zygote.ignore() do\n",
    "#         batch[\"rewards\"] + γ*next_values\n",
    "#     end\n",
    "    target = batch[\"rewards\"] + γ*next_values\n",
    "    for i = 1:size(batch[\"actions\"],2)\n",
    "        loss = loss + mse(q_values[batch[\"actions\"][i],i], target[i])\n",
    "    end\n",
    "    return loss/num_sample\n",
    "###############################################\n",
    "# implementation 2 (does not work)\n",
    "#     q_values = gather(q_values,batch[\"actions\"])\n",
    "#     target = Flux.Zygote.ignore() do\n",
    "#         batch[\"rewards\"] + γ*next_values\n",
    "#     end\n",
    "#     loss = mse(q_values, target)\n",
    "#     return loss\n",
    "###############################################\n",
    "# implementation 3 (does not work)\n",
    "#     next_q_unfiltered = learner(batch[\"next_states\"])\n",
    "#     target_unfiltered = Flux.Zygote.ignore() do\n",
    "#         broadcast(+, batch[\"rewards\"], γ * next_q_unfiltered)\n",
    "#     end\n",
    "#     loss_unfiltered = mse(q_values,target_unfiltered;agg=identity)\n",
    "#     for i = 1:num_sample\n",
    "#         loss = loss + loss_unfiltered[batch[\"actions\"][i],i]\n",
    "#     end\n",
    "#     println(loss)\n",
    "#     return loss / num_sample\n",
    "end\n",
    "\n",
    "function update_learner(learner, batch)\n",
    "    grad = Flux.gradient(() -> value_loss(learner, batch), Flux.params(learner))\n",
    "    Flux.update!(optimizer, Flux.params(learner), grad)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb11a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, policy, loss, update step\n",
    "learner = Chain(\n",
    "    Dense(ns, 128, relu; init = glorot_uniform(rng)),\n",
    "    Dense(128, 128, relu; init = glorot_uniform(rng)),\n",
    "    Dense(128, na; init = glorot_uniform(rng)),\n",
    ");\n",
    "optimizer = ADAM();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "556884b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interact with env to collect data and do the update steps\n",
    "policy = EpsilonGreedyPolicy\n",
    "stop_criterion = StopAfterEpisode(15000)\n",
    "total_rewards = Array{Float64}(undef, 1, 1)\n",
    "step_counter = 0\n",
    "max_step = 1e5\n",
    "\n",
    "while true\n",
    "    reset!(env)\n",
    "    episode_reward = 0\n",
    "    states = Array(state(env))\n",
    "    actions = Array{Int32}(undef, 1, 1)\n",
    "    rewards = Array{Float64}(undef, 1, 1)\n",
    "    \n",
    "    while !is_terminated(env)\n",
    "        #env |> policy |> env\n",
    "        action = policy(state(env), learner, step_counter, max_step)[1]\n",
    "        step_counter = step_counter +1\n",
    "        env(action)\n",
    "        \n",
    "        states = [states state(env)]\n",
    "        actions = [actions action]\n",
    "        rewards = [rewards reward(env)]\n",
    "        episode_reward += reward(env)\n",
    "        #stop_criterion(policy, env) && return # stop criterion: max episodes\n",
    "    end\n",
    "    # end of an episode\n",
    "    # processing the data\n",
    "    next_states = states[:,2:end]\n",
    "    states = states[:,1:end-1]\n",
    "    rewards = rewards[:,2:end]\n",
    "    actions = actions[:,2:end]\n",
    "    action_index = [(0,0)]\n",
    "    for i = 1:length(actions)\n",
    "        action_index = [action_index (actions[i],i)]\n",
    "    end\n",
    "    action_index = action_index[:,2:end]\n",
    "    # TODO: use named tuple instead of dictionary for performance\n",
    "    batch = Dict(\"states\"=>states, \"actions\"=>actions, \"rewards\"=>rewards,\n",
    "                 \"next_states\"=>next_states, \"action_mask\"=>action_index)\n",
    "    \n",
    "    total_rewards = [total_rewards episode_reward]\n",
    "    step_counter >= max_step && break # stop criterion: max steps\n",
    "    \n",
    "    # update steps\n",
    "    for i = 1:3\n",
    "        update_learner(learner, batch)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6a8f28d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             ⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[97;1mTotal reward per episode\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n",
       "             \u001b[38;5;8m┌────────────────────────────────────────┐\u001b[0m \n",
       "         \u001b[38;5;8m200\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀\u001b[38;5;2m⣷\u001b[0m\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⣇\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⡏\u001b[0m\u001b[38;5;2m⠁\u001b[0m\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "            \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣇\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "            \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "            \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡄\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "            \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⣧\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣾\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "            \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢠\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "            \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⡀\u001b[0m⠀⠀\u001b[38;5;2m⡀\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⡆\u001b[0m\u001b[38;5;2m⢸\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "   Score    \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀\u001b[38;5;2m⡀\u001b[0m⠀⠀⠀\u001b[38;5;2m⡄\u001b[0m⠀⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣷\u001b[0m\u001b[38;5;2m⣿\u001b[0m⠀\u001b[38;5;2m⣼\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "            \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⡀\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⢰\u001b[0m⠀\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⣦\u001b[0m\u001b[38;5;2m⣾\u001b[0m\u001b[38;5;2m⣸\u001b[0m\u001b[38;5;2m⣾\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m⠀\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣾\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⠃\u001b[0m⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "            \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀\u001b[38;5;2m⣇\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣧\u001b[0m\u001b[38;5;2m⣧\u001b[0m\u001b[38;5;2m⣸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣶\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "            \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⡄\u001b[0m\u001b[38;5;2m⣼\u001b[0m\u001b[38;5;2m⣇\u001b[0m⠀\u001b[38;5;2m⣄\u001b[0m\u001b[38;5;2m⢀\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣇\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "            \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⣷\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣾\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⣷\u001b[0m\u001b[38;5;2m⣾\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "            \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣷\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "            \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⡿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "           \u001b[38;5;8m0\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠙\u001b[0m\u001b[38;5;2m⠙\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠋\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠋\u001b[0m\u001b[38;5;2m⠃\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠋\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠋\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠋\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠙\u001b[0m\u001b[38;5;2m⠙\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠋\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠙\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠋\u001b[0m\u001b[38;5;2m⠃\u001b[0m\u001b[38;5;2m⠋\u001b[0m\u001b[38;5;2m⠛\u001b[0m\u001b[38;5;2m⠛\u001b[0m⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "             \u001b[38;5;8m└────────────────────────────────────────┘\u001b[0m \n",
       "             ⠀\u001b[38;5;8m0\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m2000\u001b[0m⠀ \n",
       "             ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot the episodes\n",
    "p = lineplot(total_rewards[2:end], title=\"Total reward per episode\", xlabel=\"Episode\", ylabel=\"Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abd860",
   "metadata": {},
   "source": [
    "### Adding experience replay buffer\n",
    "\n",
    "We have already implemented a basic DQN agent using on-policy data (in the first update repetition) with epsilon greedy exploration, the policy can already pick up effectively. However, due to the violation of some standard machine learning assumptions (samples are not i.i.d. and drawn from a stationary distribution), the learned policy can be unstable ('catastrophic forgetting'). Now we add an huge experience replay buffer to make our data distribution more 'stationary'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5414c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_buffer = Chain(\n",
    "    Dense(ns, 128, relu; init = glorot_uniform(rng)),\n",
    "    Dense(128, 128, relu; init = glorot_uniform(rng)),\n",
    "    Dense(128, na; init = glorot_uniform(rng)),\n",
    ");\n",
    "optimizer = ADAM();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2744f59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learn_from_batch (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_buffer(buffer, batch, buffer_size)\n",
    "    buffer[\"states\"] = [buffer[\"states\"] batch[\"states\"]]\n",
    "    buffer[\"actions\"] = [buffer[\"actions\"] batch[\"actions\"]]\n",
    "    buffer[\"next_states\"] = [buffer[\"next_states\"] batch[\"next_states\"]]\n",
    "    buffer[\"rewards\"] = [buffer[\"rewards\"] batch[\"rewards\"]]\n",
    "    \n",
    "    # trim the old data if the buffer is full\n",
    "    if size(buffer[\"states\"],2) >= buffer_size\n",
    "        offset = size(buffer[\"states\"],2) - buffer_size\n",
    "        offset = convert(Int32, offset) + 1 # avoid 0-index\n",
    "        buffer[\"states\"] = buffer[\"states\"][:,offset:end]\n",
    "        buffer[\"actions\"] = buffer[\"actions\"][:,offset:end]\n",
    "        buffer[\"next_states\"] = buffer[\"next_states\"][:,offset:end]\n",
    "        buffer[\"rewards\"] = buffer[\"rewards\"][:,offset:end]\n",
    "    end\n",
    "    return buffer\n",
    "end\n",
    "\n",
    "function sample_buffer(buffer, batch_size)\n",
    "    idx = rand(rng, 1:size(buffer[\"states\"],2), batch_size)\n",
    "    batch = Dict(\"states\"=>Array{Float64}(undef, ns, 1), \"actions\"=>Array{Int32}(undef, 1, 1), \n",
    "              \"rewards\"=>Array{Float64}(undef, 1, 1), \"next_states\"=>Array{Float64}(undef, ns, 1))    \n",
    "    batch[\"states\"] = buffer[\"states\"][:,idx]\n",
    "    batch[\"actions\"] = buffer[\"actions\"][:,idx]\n",
    "    batch[\"next_states\"] = buffer[\"next_states\"][:,idx]\n",
    "    batch[\"rewards\"] = buffer[\"rewards\"][:,idx]\n",
    "    \n",
    "    # to include the newest trajectories\n",
    "    batch[\"states\"] = [batch[\"states\"] buffer[\"states\"][:,end-200:end]]\n",
    "    batch[\"actions\"] = [batch[\"actions\"] buffer[\"actions\"][:,end-200:end]]\n",
    "    batch[\"next_states\"] = [batch[\"next_states\"] buffer[\"next_states\"][:,end-200:end]]\n",
    "    batch[\"rewards\"] = [batch[\"rewards\"] buffer[\"rewards\"][:,end-200:end]]\n",
    "    \n",
    "    return batch\n",
    "end\n",
    "\n",
    "function learn_from_batch(learner_buffer, buffer, batch_size)\n",
    "    batch = sample_buffer(buffer, batch_size)\n",
    "    # update steps\n",
    "    for i = 1:3\n",
    "        update_learner(learner_buffer, batch)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93ef5e3",
   "metadata": {},
   "source": [
    "# TODO: debugging here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87d2197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interact with env to collect data and do the update steps\n",
    "policy = EpsilonGreedyPolicy\n",
    "#stop_criterion = StopAfterEpisode(15000)\n",
    "\n",
    "ss = nothing\n",
    "aa = nothing\n",
    "rr = nothing\n",
    "nst = nothing\n",
    "bb = nothing\n",
    "offset = nothing\n",
    "total_rewards = Array{Float64}(undef, 1, 1)\n",
    "step_counter = 0\n",
    "max_step = 2e5\n",
    "batch_size = 2048\n",
    "buffer_size = 1e5\n",
    "update_interval = 200\n",
    "buffer = Dict(\"states\"=>Array{Float64}(undef, ns, 1), \"actions\"=>Array{Int32}(undef, 1, 1), \n",
    "              \"rewards\"=>Array{Float64}(undef, 1, 1), \"next_states\"=>Array{Float64}(undef, ns, 1))\n",
    "while true\n",
    "    reset!(env)\n",
    "    episode_reward = 0\n",
    "    states = Array(state(env))\n",
    "    actions = Array{Int32}(undef, 1, 1)\n",
    "    rewards = Array{Float64}(undef, 1, 1)\n",
    "    \n",
    "    \n",
    "    while !is_terminated(env)\n",
    "        #env |> policy |> env\n",
    "        action = policy(state(env), learner_buffer, step_counter, max_step)[1]\n",
    "        step_counter = step_counter +1\n",
    "        env(action)\n",
    "        \n",
    "        states = [states state(env)]\n",
    "        actions = [actions action]\n",
    "        rewards = [rewards reward(env)]\n",
    "        episode_reward += reward(env)\n",
    "        #stop_criterion(policy, env) && return # stop criterion: max episodes\n",
    "        \n",
    "        # every `update_interval` steps do the update step\n",
    "        if mod(step_counter, update_interval) == 0 && size(buffer[\"states\"], 2) > batch_size\n",
    "            learn_from_batch(learner_buffer, buffer, batch_size)\n",
    "        end        \n",
    "    end\n",
    "    # end of an episode\n",
    "    # processing the data\n",
    "    next_states = states[:,2:end]\n",
    "    states = states[:,1:end-1]\n",
    "    rewards = rewards[:,2:end]\n",
    "    actions = actions[:,2:end]\n",
    "    episode = Dict(\"states\"=>states, \"actions\"=>actions, \"rewards\"=>rewards,\n",
    "                 \"next_states\"=>next_states)    \n",
    "    buffer = load_buffer(buffer, episode, buffer_size)\n",
    "    \n",
    "    total_rewards = [total_rewards episode_reward]\n",
    "    step_counter >= max_step && break # stop criterion: max steps\n",
    "\n",
    "    ss = states\n",
    "    aa = actions\n",
    "    rr = rewards\n",
    "    nst = next_states\n",
    "    bb = episode\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4eee6fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            ⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[97;1mTotal reward per episode\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n",
       "            \u001b[38;5;8m┌────────────────────────────────────────┐\u001b[0m \n",
       "         \u001b[38;5;8m90\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⡄\u001b[0m\u001b[38;5;2m⣷\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⢠\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⣾\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢰\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣼\u001b[0m\u001b[38;5;2m⣶\u001b[0m\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⢀\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m⠀\u001b[38;5;2m⢀\u001b[0m\u001b[38;5;2m⢠\u001b[0m⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡆\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⢸\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⡄\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "   Score   \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣾\u001b[0m\u001b[38;5;2m⣷\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣼\u001b[0m\u001b[38;5;2m⣧\u001b[0m\u001b[38;5;2m⣷\u001b[0m\u001b[38;5;2m⣿\u001b[0m⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣇\u001b[0m\u001b[38;5;2m⣼\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⡄\u001b[0m⠀\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣧\u001b[0m\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⣇\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⡄\u001b[0m⠀\u001b[38;5;2m⡆\u001b[0m⠀⠀⠀\u001b[38;5;2m⡀\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣾\u001b[0m\u001b[38;5;2m⣧\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣧\u001b[0m⠀\u001b[38;5;2m⣰\u001b[0m⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⡄\u001b[0m⠀\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⢀\u001b[0m\u001b[38;5;2m⡄\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣷\u001b[0m\u001b[38;5;2m⣴\u001b[0m\u001b[38;5;2m⡄\u001b[0m\u001b[38;5;2m⡀\u001b[0m\u001b[38;5;2m⣄\u001b[0m⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⡿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "          \u001b[38;5;8m0\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
       "            \u001b[38;5;8m└────────────────────────────────────────┘\u001b[0m \n",
       "            ⠀\u001b[38;5;8m0\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m40000\u001b[0m⠀ \n",
       "            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot the episodes\n",
    "p = lineplot(total_rewards[2:end], title=\"Total reward per episode\", xlabel=\"Episode\", ylabel=\"Score\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
