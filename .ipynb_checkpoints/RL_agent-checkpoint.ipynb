{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe242f8",
   "metadata": {},
   "source": [
    "## 1. A gentle example of using ReinforcementLearning.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ec02870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg;\n",
    "# uncomment the following if you have not installed them\n",
    "# Pkg.add(\"ReinforcementLearning\");\n",
    "# Pkg.add(\"Flux\");\n",
    "# Pkg.add(\"StableRNGs\");\n",
    "# Pkg.add(\"Distributions\");\n",
    "using Flux: InvDecay;\n",
    "using ReinforcementLearning;\n",
    "using StableRNGs;\n",
    "using Flux;\n",
    "using Flux.Losses;\n",
    "using Distributions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb0dd9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# RandomWalk1D\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                Value |\n",
       "|:----------------- | --------------------:|\n",
       "| NumAgentStyle     |        SingleAgent() |\n",
       "| DynamicStyle      |         Sequential() |\n",
       "| InformationStyle  | PerfectInformation() |\n",
       "| ChanceStyle       |      Deterministic() |\n",
       "| RewardStyle       |     TerminalReward() |\n",
       "| UtilityStyle      |         GeneralSum() |\n",
       "| ActionStyle       |   MinimalActionSet() |\n",
       "| StateStyle        | Observation{Int64}() |\n",
       "| DefaultStateStyle | Observation{Int64}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Base.OneTo(7)`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Base.OneTo(2)`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "4\n",
       "```\n"
      ],
      "text/plain": [
       "# RandomWalk1D\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                Value |\n",
       "|:----------------- | --------------------:|\n",
       "| NumAgentStyle     |        SingleAgent() |\n",
       "| DynamicStyle      |         Sequential() |\n",
       "| InformationStyle  | PerfectInformation() |\n",
       "| ChanceStyle       |      Deterministic() |\n",
       "| RewardStyle       |     TerminalReward() |\n",
       "| UtilityStyle      |         GeneralSum() |\n",
       "| ActionStyle       |   MinimalActionSet() |\n",
       "| StateStyle        | Observation{Int64}() |\n",
       "| DefaultStateStyle | Observation{Int64}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Base.OneTo(7)`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Base.OneTo(2)`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "4\n",
       "```\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RandomWalk1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f8f2a",
   "metadata": {},
   "source": [
    "### random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d87c63e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = action_space(env)\n",
    "while true\n",
    "    env(rand(A))\n",
    "    is_terminated(env) && break\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61151f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[97;1mTotal reward per episode\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n",
      "            \u001b[38;5;8m┌────────────────────────────────────────┐\u001b[0m \n",
      "          \u001b[38;5;8m1\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⣷\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⣇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⣸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⢻\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢰\u001b[0m\u001b[38;5;2m⢹\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡏\u001b[0m\u001b[38;5;2m⡆\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⠈\u001b[0m\u001b[38;5;2m⡆\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢰\u001b[0m\u001b[38;5;2m⠁\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⡎\u001b[0m⠀\u001b[38;5;2m⢇\u001b[0m⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⢀\u001b[0m\u001b[38;5;2m⠇\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀\u001b[38;5;2m⠸\u001b[0m\u001b[38;5;2m⡀\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀\u001b[38;5;2m⠈\u001b[0m\u001b[38;5;2m⡆\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀\u001b[38;5;2m⠸\u001b[0m\u001b[38;5;2m⡀\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢀\u001b[0m\u001b[38;5;2m⠇\u001b[0m⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀\u001b[38;5;2m⡸\u001b[0m⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⡎\u001b[0m⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀\u001b[38;5;2m⢱\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "   Score   \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠤⠤\u001b[38;5;2m⡧\u001b[0m⠤⠤⠤\u001b[38;5;2m⢼\u001b[0m⠤⠤⠤⠤\u001b[38;5;2m⡧\u001b[0m⠤⠤⠤\u001b[38;5;2m⢧\u001b[0m⠤⠤⠤⠤⠤⠤⠤⠤\u001b[38;5;2m⡼\u001b[0m⠤⠤⠤\u001b[38;5;2m⢼\u001b[0m⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⠃\u001b[0m⠀⠀⠀\u001b[38;5;2m⠸\u001b[0m\u001b[38;5;2m⡀\u001b[0m⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀\u001b[38;5;2m⡸\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⢇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⢣\u001b[0m⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⢇\u001b[0m⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡸\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⢀\u001b[0m\u001b[38;5;2m⠇\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⠃\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⠘\u001b[0m\u001b[38;5;2m⡄\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⠘\u001b[0m\u001b[38;5;2m⡄\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⠃\u001b[0m⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⡜\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢇\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "         \u001b[38;5;8m-1\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⠇\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢱\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⡎\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⠸\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "            \u001b[38;5;8m└────────────────────────────────────────┘\u001b[0m \n",
      "            ⠀\u001b[38;5;8m1\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m10\u001b[0m⠀ \n",
      "            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TotalRewardPerEpisode([-1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0], 0.0, true)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\n",
    "    RandomPolicy(),\n",
    "    RandomWalk1D(),\n",
    "    StopAfterEpisode(10),\n",
    "    TotalRewardPerEpisode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f083a2",
   "metadata": {},
   "source": [
    "### tabular policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92206641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tabular policy\n",
    "S = state_space(env);\n",
    "A = action_space(env);\n",
    "NS, NA = length(S),A;\n",
    "tabular_policy = TabularPolicy(;table=Dict(zip(1:NS, fill(2,NS))));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e83dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[97;1mTotal reward per episode\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n",
      "           \u001b[38;5;8m┌────────────────────────────────────────┐\u001b[0m \n",
      "         \u001b[38;5;8m2\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "   Score  \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "          \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "         \u001b[38;5;8m0\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m└────────────────────────────────────────┘\u001b[0m \n",
      "           ⠀\u001b[38;5;8m1\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m10\u001b[0m⠀ \n",
      "           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TotalRewardPerEpisode([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.0, true)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\n",
    "   tabular_policy,\n",
    "   RandomWalk1D(),\n",
    "   StopAfterEpisode(10),\n",
    "   TotalRewardPerEpisode()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b22067",
   "metadata": {},
   "source": [
    "### `QBasedPolicy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23addc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typename(QBasedPolicy)\n",
       "├─ learner => typename(MonteCarloLearner)\n",
       "│  ├─ approximator => typename(TabularApproximator)\n",
       "│  │  ├─ table => 2×7 Matrix{Float64}\n",
       "│  │  └─ optimizer => typename(InvDecay)\n",
       "│  │     ├─ gamma => 1.0\n",
       "│  │     └─ state => typename(IdDict)\n",
       "│  ├─ γ => 1.0\n",
       "│  ├─ kind => typename(ReinforcementLearningZoo.FirstVisit)\n",
       "│  └─ sampling => typename(ReinforcementLearningZoo.NoSampling)\n",
       "└─ explorer => typename(EpsilonGreedyExplorer)\n",
       "   ├─ ϵ_stable => 0.1\n",
       "   ├─ ϵ_init => 1.0\n",
       "   ├─ warmup_steps => 0\n",
       "   ├─ decay_steps => 0\n",
       "   ├─ step => 1\n",
       "   ├─ rng => typename(Random._GLOBAL_RNG)\n",
       "   └─ is_training => true\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `MonteCarloLearner + EpsilonGreedyExplorer`\n",
    "policy = QBasedPolicy(\n",
    "   learner = MonteCarloLearner(;\n",
    "           approximator=TabularQApproximator(\n",
    "               ;n_state = NS,\n",
    "               n_action = NA,\n",
    "               opt = InvDecay(1.0)\n",
    "           )\n",
    "       ),\n",
    "   explorer = EpsilonGreedyExplorer(0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03daa1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[97;1mTotal reward per episode\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n",
      "            \u001b[38;5;8m┌────────────────────────────────────────┐\u001b[0m \n",
      "          \u001b[38;5;8m0\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "   Score   \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;2m⠤\u001b[0m\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "         \u001b[38;5;8m-2\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "            \u001b[38;5;8m└────────────────────────────────────────┘\u001b[0m \n",
      "            ⠀\u001b[38;5;8m1\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m10\u001b[0m⠀ \n",
      "            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TotalRewardPerEpisode([-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], 0.0, true)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\n",
    "   policy,\n",
    "   RandomWalk1D(),\n",
    "   StopAfterEpisode(10),\n",
    "   TotalRewardPerEpisode()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de87e2",
   "metadata": {},
   "source": [
    "### wrap the policy + trajectory into the 'agent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "940bd60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typename(Agent)\n",
       "├─ policy => typename(QBasedPolicy)\n",
       "│  ├─ learner => typename(MonteCarloLearner)\n",
       "│  │  ├─ approximator => typename(TabularApproximator)\n",
       "│  │  │  ├─ table => 2×7 Matrix{Float64}\n",
       "│  │  │  └─ optimizer => typename(InvDecay)\n",
       "│  │  │     ├─ gamma => 1.0\n",
       "│  │  │     └─ state => typename(IdDict)\n",
       "│  │  ├─ γ => 1.0\n",
       "│  │  ├─ kind => typename(ReinforcementLearningZoo.FirstVisit)\n",
       "│  │  └─ sampling => typename(ReinforcementLearningZoo.NoSampling)\n",
       "│  └─ explorer => typename(EpsilonGreedyExplorer)\n",
       "│     ├─ ϵ_stable => 0.1\n",
       "│     ├─ ϵ_init => 1.0\n",
       "│     ├─ warmup_steps => 0\n",
       "│     ├─ decay_steps => 0\n",
       "│     ├─ step => 31\n",
       "│     ├─ rng => typename(Random._GLOBAL_RNG)\n",
       "│     └─ is_training => true\n",
       "└─ trajectory => typename(Trajectory)\n",
       "   └─ traces => typename(NamedTuple)\n",
       "      ├─ state => 0-element Vector{Int64}\n",
       "      ├─ action => 0-element Vector{Int64}\n",
       "      ├─ reward => 0-element Vector{Float32}\n",
       "      └─ terminal => 0-element Vector{Bool}\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(policy=policy, trajectory=VectorSARTTrajectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cddd4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[97;1mTotal reward per episode\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n",
      "            \u001b[38;5;8m┌────────────────────────────────────────┐\u001b[0m \n",
      "          \u001b[38;5;8m1\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⡏\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;2m⠉\u001b[0m\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⠃\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⡎\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀\u001b[38;5;2m⡸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "   Score   \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠤⠤\u001b[38;5;2m⡧\u001b[0m⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀\u001b[38;5;2m⢠\u001b[0m\u001b[38;5;2m⠃\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⢀\u001b[0m\u001b[38;5;2m⠇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "           \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⡜\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "         \u001b[38;5;8m-1\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "            \u001b[38;5;8m└────────────────────────────────────────┘\u001b[0m \n",
      "            ⠀\u001b[38;5;8m1\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m10\u001b[0m⠀ \n",
      "            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TotalRewardPerEpisode([-1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.0, true)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(agent, env, StopAfterEpisode(10), TotalRewardPerEpisode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53760509",
   "metadata": {},
   "source": [
    "## 2. PPO algorithm for pendulum problem (built-in experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c66a6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.7/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.7/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.7/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.7/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "function RL.Experiment(\n",
    "    ::Val{:JuliaRL},\n",
    "    ::Val{:PPO},\n",
    "    ::Val{:Pendulum},\n",
    "    ::Nothing;\n",
    "    save_dir = nothing,\n",
    "    seed = 123,\n",
    ")\n",
    "    rng = StableRNG(seed)\n",
    "    inner_env = PendulumEnv(T = Float32, rng = rng)\n",
    "    A = action_space(inner_env)\n",
    "    low = A.left\n",
    "    high = A.right\n",
    "    ns = length(state(inner_env))\n",
    "\n",
    "    N_ENV = 8\n",
    "    UPDATE_FREQ = 2048\n",
    "    env = MultiThreadEnv([\n",
    "        PendulumEnv(T = Float32, rng = StableRNG(hash(seed + i))) |>\n",
    "        env -> ActionTransformedEnv(env, action_mapping = x -> clamp(x * 2, low, high)) for i in 1:N_ENV\n",
    "    ])\n",
    "\n",
    "    init = glorot_uniform(rng)\n",
    "\n",
    "    agent = Agent(\n",
    "        policy = PPOPolicy(\n",
    "            approximator = ActorCritic(\n",
    "                actor = GaussianNetwork(\n",
    "                    pre = Chain(\n",
    "                        Dense(ns, 64, relu; init = glorot_uniform(rng)),\n",
    "                        Dense(64, 64, relu; init = glorot_uniform(rng)),\n",
    "                    ),\n",
    "                    μ = Chain(Dense(64, 1, tanh; init = glorot_uniform(rng)), vec),\n",
    "                    logσ = Chain(Dense(64, 1; init = glorot_uniform(rng)), vec),\n",
    "                ),\n",
    "                critic = Chain(\n",
    "                    Dense(ns, 64, relu; init = glorot_uniform(rng)),\n",
    "                    Dense(64, 64, relu; init = glorot_uniform(rng)),\n",
    "                    Dense(64, 1; init = glorot_uniform(rng)),\n",
    "                ),\n",
    "                optimizer = ADAM(3e-4),\n",
    "            ) |> gpu,\n",
    "            γ = 0.99f0,\n",
    "            λ = 0.95f0,\n",
    "            clip_range = 0.2f0,\n",
    "            max_grad_norm = 0.5f0,\n",
    "            n_epochs = 10,\n",
    "            n_microbatches = 32,\n",
    "            actor_loss_weight = 1.0f0,\n",
    "            critic_loss_weight = 0.5f0,\n",
    "            entropy_loss_weight = 0.00f0,\n",
    "            dist = Normal,\n",
    "            rng = rng,\n",
    "            update_freq = UPDATE_FREQ,\n",
    "        ),\n",
    "        trajectory = PPOTrajectory(;\n",
    "            capacity = UPDATE_FREQ,\n",
    "            state = Matrix{Float32} => (ns, N_ENV),\n",
    "            action = Vector{Float32} => (N_ENV,),\n",
    "            action_log_prob = Vector{Float32} => (N_ENV,),\n",
    "            reward = Vector{Float32} => (N_ENV,),\n",
    "            terminal = Vector{Bool} => (N_ENV,),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    stop_condition = StopAfterStep(50_000, is_show_progress=!haskey(ENV, \"CI\"))\n",
    "    hook = TotalBatchRewardPerEpisode(N_ENV)\n",
    "    Experiment(agent, env, stop_condition, hook, \"# Play Pendulum with PPO\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f4b9978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.7/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.7/Manifest.toml`\n",
      "┌ Info: The GPU function is being called but the GPU is not accessible. \n",
      "│ Defaulting back to the CPU. (No action is required if you want to run on the CPU).\n",
      "└ @ Flux /home/richard/.julia/packages/Flux/7nTyc/src/functor.jl:187\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\section{Play Pendulum with PPO}\n"
      ],
      "text/markdown": [
       "# Play Pendulum with PPO\n"
      ],
      "text/plain": [
       "\u001b[1m  Play Pendulum with PPO\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:50\u001b[39m9:52\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ⠀⠀⠀⠀⠀⠀⠀\u001b[97;1mAvg total reward per episode\u001b[0m⠀⠀⠀⠀⠀⠀⠀ \n",
      "               \u001b[38;5;8m┌────────────────────────────────────────┐\u001b[0m \n",
      "             \u001b[38;5;8m0\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;1m⢠\u001b[0m\u001b[38;5;1m⣾\u001b[0m\u001b[38;5;1m⣷\u001b[0m\u001b[38;5;1m⡿\u001b[0m\u001b[38;5;1m⣶\u001b[0m\u001b[38;5;1m⣿\u001b[0m\u001b[38;5;1m⣿\u001b[0m\u001b[38;5;1m⣴\u001b[0m\u001b[38;5;1m⣷\u001b[0m\u001b[38;5;1m⠃\u001b[0m⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "              \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;1m⢠\u001b[0m\u001b[38;5;1m⣴\u001b[0m\u001b[38;5;1m⣸\u001b[0m\u001b[38;5;1m⠟\u001b[0m\u001b[38;5;3m⣯\u001b[0m\u001b[38;5;7m⣧\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;3m⣿\u001b[0m\u001b[38;5;6m⣶\u001b[0m\u001b[38;5;7m⣾\u001b[0m\u001b[38;5;3m⣸\u001b[0m\u001b[38;5;2m⡆\u001b[0m⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "              \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;1m⣼\u001b[0m\u001b[38;5;1m⣿\u001b[0m\u001b[38;5;3m⣿\u001b[0m\u001b[38;5;2m⣿\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;6m⣧\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;6m⢻\u001b[0m\u001b[38;5;4m⡆\u001b[0m⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "              \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;1m⢀\u001b[0m\u001b[38;5;1m⢀\u001b[0m\u001b[38;5;1m⢀\u001b[0m\u001b[38;5;3m⣿\u001b[0m\u001b[38;5;3m⣿\u001b[0m\u001b[38;5;7m⣾\u001b[0m\u001b[38;5;6m⢫\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;4m⣿\u001b[0m\u001b[38;5;4m⠻\u001b[0m\u001b[38;5;4m⢿\u001b[0m\u001b[38;5;4m⡟\u001b[0m\u001b[38;5;4m⢻\u001b[0m\u001b[38;5;4m⣿\u001b[0m\u001b[38;5;4m⠇\u001b[0m⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "              \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;1m⡀\u001b[0m\u001b[38;5;1m⢀\u001b[0m\u001b[38;5;1m⡀\u001b[0m\u001b[38;5;1m⢸\u001b[0m\u001b[38;5;1m⣿\u001b[0m\u001b[38;5;1m⣾\u001b[0m\u001b[38;5;1m⣿\u001b[0m\u001b[38;5;7m⣿\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;4m⣿\u001b[0m\u001b[38;5;4m⡏\u001b[0m\u001b[38;5;4m⠁\u001b[0m⠀\u001b[38;5;4m⠸\u001b[0m\u001b[38;5;4m⡇\u001b[0m\u001b[38;5;4m⠘\u001b[0m\u001b[38;5;4m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "              \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀\u001b[38;5;1m⢀\u001b[0m⠀⠀⠀⠀\u001b[38;5;1m⢰\u001b[0m⠀\u001b[38;5;1m⣠\u001b[0m⠀\u001b[38;5;1m⡀\u001b[0m\u001b[38;5;1m⡀\u001b[0m\u001b[38;5;1m⣧\u001b[0m\u001b[38;5;1m⣧\u001b[0m\u001b[38;5;1m⢸\u001b[0m\u001b[38;5;1m⣿\u001b[0m\u001b[38;5;3m⣿\u001b[0m\u001b[38;5;3m⣿\u001b[0m\u001b[38;5;3m⣧\u001b[0m\u001b[38;5;3m⣷\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;4m⣿\u001b[0m\u001b[38;5;4m⢸\u001b[0m⠀⠀⠀⠀⠀⠀\u001b[38;5;4m⠁\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "              \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;1m⣄\u001b[0m\u001b[38;5;1m⣆\u001b[0m\u001b[38;5;1m⢠\u001b[0m\u001b[38;5;1m⣼\u001b[0m\u001b[38;5;1m⣦\u001b[0m\u001b[38;5;1m⢰\u001b[0m\u001b[38;5;1m⣴\u001b[0m\u001b[38;5;1m⣼\u001b[0m\u001b[38;5;1m⣿\u001b[0m\u001b[38;5;1m⣷\u001b[0m\u001b[38;5;1m⣿\u001b[0m\u001b[38;5;1m⡷\u001b[0m\u001b[38;5;3m⣿\u001b[0m\u001b[38;5;3m⣿\u001b[0m\u001b[38;5;3m⡿\u001b[0m\u001b[38;5;3m⣝\u001b[0m\u001b[38;5;3m⢧\u001b[0m\u001b[38;5;3m⣾\u001b[0m\u001b[38;5;7m⣿\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;6m⡏\u001b[0m\u001b[38;5;6m⣯\u001b[0m\u001b[38;5;4m⣿\u001b[0m\u001b[38;5;4m⡇\u001b[0m\u001b[38;5;4m⠏\u001b[0m\u001b[38;5;4m⠈\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "   Score      \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;1m⣿\u001b[0m\u001b[38;5;3m⣼\u001b[0m\u001b[38;5;3m⢚\u001b[0m\u001b[38;5;1m⠋\u001b[0m\u001b[38;5;1m⠿\u001b[0m\u001b[38;5;1m⣿\u001b[0m\u001b[38;5;3m⣿\u001b[0m\u001b[38;5;3m⣿\u001b[0m\u001b[38;5;3m⣇\u001b[0m\u001b[38;5;7m⣿\u001b[0m\u001b[38;5;7m⣿\u001b[0m\u001b[38;5;6m⣶\u001b[0m\u001b[38;5;6m⡿\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;6m⣟\u001b[0m\u001b[38;5;6m⢿\u001b[0m\u001b[38;5;6m⣯\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;4m⢿\u001b[0m\u001b[38;5;4m⣿\u001b[0m\u001b[38;5;4m⣿\u001b[0m\u001b[38;5;4m⠛\u001b[0m\u001b[38;5;4m⢻\u001b[0m\u001b[38;5;4m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "              \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;1m⠏\u001b[0m\u001b[38;5;7m⣿\u001b[0m\u001b[38;5;6m⣾\u001b[0m\u001b[38;5;2m⣰\u001b[0m\u001b[38;5;2m⣦\u001b[0m\u001b[38;5;1m⠹\u001b[0m\u001b[38;5;3m⣿\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;6m⡿\u001b[0m\u001b[38;5;6m⡿\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;4m⣷\u001b[0m\u001b[38;5;4m⠿\u001b[0m\u001b[38;5;4m⠟\u001b[0m\u001b[38;5;4m⠟\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;4m⠛\u001b[0m\u001b[38;5;4m⠘\u001b[0m\u001b[38;5;4m⢿\u001b[0m\u001b[38;5;4m⡇\u001b[0m⠀\u001b[38;5;4m⠘\u001b[0m\u001b[38;5;4m⠁\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "              \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;2m⣾\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;6m⢏\u001b[0m\u001b[38;5;6m⢏\u001b[0m\u001b[38;5;2m⢻\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;4m⣧\u001b[0m\u001b[38;5;4m⡇\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;4m⠁\u001b[0m⠀⠀⠀\u001b[38;5;4m⢿\u001b[0m\u001b[38;5;4m⡏\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "              \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;6m⢁\u001b[0m\u001b[38;5;4m⣿\u001b[0m\u001b[38;5;4m⣿\u001b[0m\u001b[38;5;4m⢸\u001b[0m\u001b[38;5;4m⣾\u001b[0m\u001b[38;5;6m⡞\u001b[0m\u001b[38;5;6m⣿\u001b[0m\u001b[38;5;4m⣿\u001b[0m\u001b[38;5;4m⠿\u001b[0m\u001b[38;5;4m⠁\u001b[0m\u001b[38;5;4m⣿\u001b[0m⠀⠀⠀⠀\u001b[38;5;4m⠈\u001b[0m\u001b[38;5;4m⠁\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "              \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;4m⣾\u001b[0m\u001b[38;5;4m⣿\u001b[0m\u001b[38;5;4m⠙\u001b[0m\u001b[38;5;4m⡏\u001b[0m\u001b[38;5;4m⠇\u001b[0m\u001b[38;5;4m⢿\u001b[0m\u001b[38;5;4m⡿\u001b[0m\u001b[38;5;4m⠋\u001b[0m⠀⠀\u001b[38;5;4m⠟\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "              \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m\u001b[38;5;4m⠻\u001b[0m⠀⠀⠀⠀\u001b[38;5;4m⠘\u001b[0m\u001b[38;5;4m⠇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "              \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "         \u001b[38;5;8m-2000\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m\n",
      "               \u001b[38;5;8m└────────────────────────────────────────┘\u001b[0m \n",
      "               ⠀\u001b[38;5;8m0\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m300\u001b[0m⠀ \n",
      "               ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\section{Play Pendulum with PPO}\n"
      ],
      "text/markdown": [
       "# Play Pendulum with PPO\n"
      ],
      "text/plain": [
       "\u001b[1m  Play Pendulum with PPO\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "typename(Experiment)\n",
       "├─ policy => typename(Agent)\n",
       "│  ├─ policy => typename(PPOPolicy)\n",
       "│  │  ├─ approximator => typename(ActorCritic)\n",
       "│  │  │  ├─ actor => typename(GaussianNetwork)\n",
       "│  │  │  │  ├─ pre => typename(Chain)\n",
       "│  │  │  │  │  └─ layers\n",
       "│  │  │  │  │     ├─ 1\n",
       "│  │  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │  │     │     ├─ weight => 64×3 Matrix{Float32}\n",
       "│  │  │  │  │     │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │  │  │     └─ 2\n",
       "│  │  │  │  │        └─ typename(Dense)\n",
       "│  │  │  │  │           ├─ weight => 64×64 Matrix{Float32}\n",
       "│  │  │  │  │           ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │  │  │           └─ σ => typename(typeof(relu))\n",
       "│  │  │  │  ├─ μ => typename(Chain)\n",
       "│  │  │  │  │  └─ layers\n",
       "│  │  │  │  │     ├─ 1\n",
       "│  │  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │  │     │     ├─ weight => 1×64 Matrix{Float32}\n",
       "│  │  │  │  │     │     ├─ bias => 1-element Vector{Float32}\n",
       "│  │  │  │  │     │     └─ σ => typename(typeof(tanh))\n",
       "│  │  │  │  │     └─ 2\n",
       "│  │  │  │  │        └─ typename(typeof(vec))\n",
       "│  │  │  │  ├─ logσ => typename(Chain)\n",
       "│  │  │  │  │  └─ layers\n",
       "│  │  │  │  │     ├─ 1\n",
       "│  │  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │  │     │     ├─ weight => 1×64 Matrix{Float32}\n",
       "│  │  │  │  │     │     ├─ bias => 1-element Vector{Float32}\n",
       "│  │  │  │  │     │     └─ σ => typename(typeof(identity))\n",
       "│  │  │  │  │     └─ 2\n",
       "│  │  │  │  │        └─ typename(typeof(vec))\n",
       "│  │  │  │  ├─ min_σ => 0.0\n",
       "│  │  │  │  ├─ max_σ => Inf\n",
       "│  │  │  │  └─ normalizer => typename(typeof(tanh))\n",
       "│  │  │  ├─ critic => typename(Chain)\n",
       "│  │  │  │  └─ layers\n",
       "│  │  │  │     ├─ 1\n",
       "│  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │     │     ├─ weight => 64×3 Matrix{Float32}\n",
       "│  │  │  │     │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │  │     ├─ 2\n",
       "│  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │     │     ├─ weight => 64×64 Matrix{Float32}\n",
       "│  │  │  │     │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │  │     └─ 3\n",
       "│  │  │  │        └─ typename(Dense)\n",
       "│  │  │  │           ├─ weight => 1×64 Matrix{Float32}\n",
       "│  │  │  │           ├─ bias => 1-element Vector{Float32}\n",
       "│  │  │  │           └─ σ => typename(typeof(identity))\n",
       "│  │  │  └─ optimizer => typename(ADAM)\n",
       "│  │  │     ├─ eta => 0.0003\n",
       "│  │  │     ├─ beta\n",
       "│  │  │     │  ├─ 1\n",
       "│  │  │     │  │  └─ 0.9\n",
       "│  │  │     │  └─ 2\n",
       "│  │  │     │     └─ 0.999\n",
       "│  │  │     ├─ epsilon => 1.0e-8\n",
       "│  │  │     └─ state => typename(IdDict)\n",
       "│  │  ├─ γ => 0.99\n",
       "│  │  ├─ λ => 0.95\n",
       "│  │  ├─ clip_range => 0.2\n",
       "│  │  ├─ max_grad_norm => 0.5\n",
       "│  │  ├─ n_microbatches => 32\n",
       "│  │  ├─ n_epochs => 10\n",
       "│  │  ├─ actor_loss_weight => 1.0\n",
       "│  │  ├─ critic_loss_weight => 0.5\n",
       "│  │  ├─ entropy_loss_weight => 0.0\n",
       "│  │  ├─ rng => typename(StableRNGs.LehmerRNG)\n",
       "│  │  ├─ n_random_start => 0\n",
       "│  │  ├─ update_freq => 2048\n",
       "│  │  ├─ update_step => 50000\n",
       "│  │  ├─ norm => 32×10 Matrix{Float32}\n",
       "│  │  ├─ actor_loss => 32×10 Matrix{Float32}\n",
       "│  │  ├─ critic_loss => 32×10 Matrix{Float32}\n",
       "│  │  ├─ entropy_loss => 32×10 Matrix{Float32}\n",
       "│  │  └─ loss => 32×10 Matrix{Float32}\n",
       "│  └─ trajectory => typename(Trajectory)\n",
       "│     └─ traces => typename(NamedTuple)\n",
       "│        ├─ action_log_prob => 8×2049 CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}\n",
       "│        ├─ state => 3×8×2049 CircularArrayBuffers.CircularArrayBuffer{Float32, 3, Array{Float32, 3}}\n",
       "│        ├─ action => 8×2049 CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}\n",
       "│        ├─ reward => 8×2048 CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}\n",
       "│        └─ terminal => 8×2048 CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}\n",
       "├─ env => typename(MultiThreadEnv)\n",
       "├─ stop_condition => typename(StopAfterStep)\n",
       "│  ├─ step => 50000\n",
       "│  ├─ cur => 50001\n",
       "│  └─ progress => typename(ProgressMeter.Progress)\n",
       "├─ hook => typename(TotalBatchRewardPerEpisode)\n",
       "│  ├─ rewards => 8-element Vector{Vector{Float64}}\n",
       "│  ├─ reward => 8-element Vector{Float64}\n",
       "│  └─ is_display_on_exit => true\n",
       "└─ description => \"# Play Pendulum with PPO\"\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endPkg.add(\"Plots\")\n",
    "using Plots\n",
    "using Statistics\n",
    "ex = E`JuliaRL_PPO_Pendulum`\n",
    "run(ex)\n",
    "# n = minimum(map(length, ex.hook.rewards))\n",
    "# m = mean([@view(x[1:n]) for x in ex.hook.rewards])\n",
    "# s = std([@view(x[1:n]) for x in ex.hook.rewards])\n",
    "# plot(m,ribbon=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8b1a4",
   "metadata": {},
   "source": [
    "## 3. Break down the experiment running implementation\n",
    "\n",
    "[reference: JuliaRinforcementLearning Blog](https://juliareinforcementlearning.org/blog/an_introduction_to_reinforcement_learning_jl_design_implementations_thoughts/#ol_start2_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71753530",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123;\n",
    "rng = StableRNG(seed);\n",
    "env = CartPoleEnv(; T = Float32);\n",
    "ns, na = length(state_space(env)), length(action_space(env));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f143670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "custom_run (generic function with 3 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function custom_run(\n",
    "        policy, \n",
    "        env, \n",
    "        stop_criterion = StopAfterEpisode(5),\n",
    "        hook = EmptyHook(),\n",
    "    )\n",
    "    step_counter = 0\n",
    "    while true\n",
    "        reset!(env)\n",
    "        #policy(PRE_EPISODE_STAGE)\n",
    "        \n",
    "        while !is_terminated(env)\n",
    "            #env |> policy |> env\n",
    "            action = policy(env)\n",
    "            step_counter = step_counter +1\n",
    "            \n",
    "            #policy(PRE_ACT_STAGE, env, action)\n",
    "            env(action)\n",
    "            \n",
    "            println(step_counter, reward(env))\n",
    "            \n",
    "            #policy(POST_ACT_STAGE, env)\n",
    "            stop_criterion(policy, env) && return\n",
    "        end\n",
    "        #policy(POST_EPISODE_STAGE)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd79a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dumm_policy (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function dumm_policy(env)\n",
    "    return 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c75830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n",
      "21.0\n",
      "31.0\n",
      "41.0\n",
      "51.0\n",
      "61.0\n",
      "71.0\n",
      "81.0\n",
      "91.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "custom_run(dumm_policy, env, StopAfterEpisode(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08604c70",
   "metadata": {},
   "source": [
    "### 4. A Q-learning Agent\n",
    "\n",
    "For simplicity, we omit the explorer here for env with small state and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d51e260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.7/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.7/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.7/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.7/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "export QBasedPolicy, TabularRandomPolicy\n",
    "\n",
    "Pkg.add(\"MacroTools\")\n",
    "Pkg.add(\"Setfield\")\n",
    "using MacroTools: @forward\n",
    "using Setfield: @set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bed58cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_learner (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = Chain(\n",
    "    Dense(ns, 128, relu; init = glorot_uniform(rng)),\n",
    "    Dense(128, 128, relu; init = glorot_uniform(rng)),\n",
    "    Dense(128, na; init = glorot_uniform(rng)),\n",
    ") |> gpu;\n",
    "optimizer = ADAM();\n",
    "\n",
    "function GreedyPolicy(states, learner)\n",
    "    logits = learner(states)\n",
    "    actions = mapslices(argmax, logits, dims=1)\n",
    "    return actions\n",
    "end\n",
    "\n",
    "function value_loss(batch)\n",
    "    # TODO: improve the inefficient loss calculation\n",
    "    num_sample = length(batch[\"actions\"])\n",
    "    loss = 0\n",
    "    γ = 0.99\n",
    "    q_values = learner(batch[\"states\"])\n",
    "    next_values = findmax(learner(bb[\"next_states\"]); dims=1)[1]\n",
    "    for i in 1:length(batch[\"actions\"])\n",
    "        target = Flux.Zygote.ignore() do\n",
    "            bb[\"rewards\"] + γ*next_values\n",
    "        end\n",
    "        loss = loss + mse(q_values[batch[\"actions\"][i],i], target)\n",
    "    end\n",
    "    return loss/num_sample\n",
    "end\n",
    "\n",
    "function update_learner(learner, batch)\n",
    "    grad = Flux.gradient(Flux.params(learner)) do\n",
    "        value_loss(batch)\n",
    "    end\n",
    "    Flux.update!(optimizer, Flux.params(learner), grad)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9cbd109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial output: Float32[-0.028421307, -0.53843874]\n",
      "updated output: Float32[0.39852583, -0.033196606]\n"
     ]
    }
   ],
   "source": [
    "# example of using Flux to update the weights\n",
    "using Flux.Losses: mse\n",
    "println(\"initial output: \", learner(state(env)))\n",
    "\n",
    "# a template update step\n",
    "function loss(states, action_label)\n",
    "    #action = argmax([model(states), model(states)], dims=2);\n",
    "    action = learner(states)\n",
    "    return sum(mse(action, action_label))\n",
    "end\n",
    "\n",
    "y = [10, 10];\n",
    "states = state(env)\n",
    "\n",
    "g = Flux.gradient(Flux.params(learner)) do\n",
    "    loss(states, y)\n",
    "end\n",
    "\n",
    "#Flux.train!(loss, Flux.params(model), [(states, y)], optimizer)\n",
    "\n",
    "for i = 1:5\n",
    "    Flux.update!(optimizer, Flux.params(learner), g)\n",
    "end\n",
    "\n",
    "println(\"updated output: \", learner(state(env)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15632f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:02\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "11.0\n",
      "9.0\n",
      "9.0\n",
      "11.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "11.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "11.0\n",
      "8.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "11.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "11.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "11.0\n",
      "9.0\n",
      "11.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "11.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "11.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "11.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "11.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "11.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "11.0\n",
      "10.0\n",
      "11.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "8.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "# online, on-policy Q-learning without experience replay buffer and exploration\n",
    "\n",
    "policy = dumm_policy\n",
    "stop_criterion = StopAfterEpisode(500)\n",
    "\n",
    "ss = nothing\n",
    "aa = nothing\n",
    "rr = nothing\n",
    "nst = nothing\n",
    "bb = nothing\n",
    "aa_idx = nothing\n",
    "\n",
    "step_counter = 0\n",
    "while true\n",
    "    reset!(env)\n",
    "    episode_reward = 0\n",
    "    #policy(PRE_EPISODE_STAGE)\n",
    "    states = Array(state(env))\n",
    "    actions = Array{Int32}(undef, 1, 1)\n",
    "    rewards = Array{Float64}(undef, 1, 1)\n",
    "    \n",
    "    while !is_terminated(env)\n",
    "        #env |> policy |> env\n",
    "        action = GreedyPolicy(state(env), learner)[1]\n",
    "        step_counter = step_counter +1\n",
    "        \n",
    "        states = [states state(env)]\n",
    "        actions = [actions action]\n",
    "        rewards = [rewards reward(env)]\n",
    "        episode_reward += reward(env)\n",
    "        \n",
    "        #policy(PRE_ACT_STAGE, env, action)\n",
    "        env(action)\n",
    "\n",
    "        #policy(POST_ACT_STAGE, env)\n",
    "        stop_criterion(policy, env) && return\n",
    "    end\n",
    "    # end of an episode\n",
    "    # processing the data\n",
    "    next_states = states[:,2:end]\n",
    "    states = states[:,1:end-1]\n",
    "    rewards = rewards[:,2:end]\n",
    "    actions = actions[:,2:end]\n",
    "    action_index = [(0,0)]\n",
    "    for i in 1:length(actions)\n",
    "        action_index = [action_index (actions[i],i)]\n",
    "    end\n",
    "    action_index = action_index[:,2:end]\n",
    "    batch = Dict(\"states\"=>states, \"actions\"=>actions, \"rewards\"=>rewards,\n",
    "                 \"next_states\"=>next_states, \"action_mask\"=>action_index)\n",
    "    \n",
    "    println(episode_reward)\n",
    "    ss = states\n",
    "    aa = actions\n",
    "    rr = rewards\n",
    "    nst = next_states\n",
    "    aa_idx = action_index\n",
    "    bb = batch\n",
    "    for i = 1:10\n",
    "        update_learner(learner, batch)\n",
    "    end\n",
    "    #policy(POST_EPISODE_STAGE)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
